cortiz20@amherst.edu
bdudziak20@amherst.edu

VALUE ITERATION
# for i in range(iterations):
    #     for state in mdp.getStates():
    #         self.values[state] = util.Counter() if self.values[state] == 0 else self.values[state]
    #
    #         if mdp.isTerminal(state):
    #             self.values[state][None] = mdp.getReward(state, None, None)
    #         else:
    #             for action in mdp.getPossibleActions(state):
    #                 state_prime = mdp.getTransitionStatesAndProbs(state, action)[0][0]
    #                 # Safety for case where state_prime hasn't been initialized
    #                 self.values[state_prime] = util.Counter() if self.values[state_prime] == 0 else self.values[state_prime]
    #
    #                 if mdp.isTerminal(state_prime):
    #                     max_action_value = mdp.getReward(state_prime, None, None)
    #                 else:
    #                     max_action_value = -99999
    #                     for action_prime in mdp.getPossibleActions(state_prime):
    #                         max_action_value = self.values[state_prime][action_prime] if self.values[state_prime][action_prime] > max_action_value else max_action_value
    #
    #                 self.values[state][action] += mdp.getReward(state, action, state_prime) + discount*max_action_value - self.values[state][action]
